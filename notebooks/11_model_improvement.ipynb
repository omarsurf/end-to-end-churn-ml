{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Model Improvement\n",
    "\n",
    "---\n",
    "\n",
    "## Objective\n",
    "\n",
    "Improve the baseline Logistic Regression model (AUC=0.828) to meet all three business targets:\n",
    "\n",
    "| Metric | Current | Target | Minimum |\n",
    "|--------|---------|--------|---------|\n",
    "| ROC-AUC | 0.828 | >= 0.85 | >= 0.80 |\n",
    "| Recall | 86.1% | >= 80% | >= 70% |\n",
    "| Precision | 46.3% | >= 50% | >= 40% |\n",
    "\n",
    "**Strategy:**\n",
    "1. Feature engineering (14 new features)\n",
    "2. Test XGBoost, LightGBM, Random Forest (tuned), Gradient Boosting (tuned)\n",
    "3. Ensemble if multiple models beat baseline\n",
    "4. Threshold re-optimization on best model\n",
    "\n",
    "**Rules:**\n",
    "- No data leakage (statistics computed on train only)\n",
    "- Same CV strategy as previous notebooks (StratifiedKFold 5, random_state=42)\n",
    "- Test set touched ONCE at the very end\n",
    "- Overfitting guards: Train-Val AUC gap < 0.05, CV std < 0.03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Setup & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    RandomizedSearchCV, GridSearchCV, StratifiedKFold,\n",
    "    cross_validate, cross_val_score\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, GradientBoostingClassifier,\n",
    "    VotingClassifier, StackingClassifier\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_curve\n",
    ")\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"Libraries loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (4225, 19)\n",
      "X_val:   (1409, 19)\n",
      "X_test:  (1409, 19)\n",
      "\n",
      "Churn rate - Train: 26.53%, Val: 26.54%, Test: 26.54%\n",
      "\n",
      "Columns: ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'tenure', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod', 'MonthlyCharges', 'TotalCharges']\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ROOT = Path.cwd().parent\n",
    "DATA_PATH = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "MODELS_PATH = PROJECT_ROOT / \"models\"\n",
    "\n",
    "# Load RAW splits (before any encoding/scaling)\n",
    "X_train = pd.read_csv(DATA_PATH / \"X_train.csv\")\n",
    "X_val = pd.read_csv(DATA_PATH / \"X_val.csv\")\n",
    "X_test = pd.read_csv(DATA_PATH / \"X_test.csv\")\n",
    "\n",
    "y_train = pd.read_csv(DATA_PATH / \"y_train.csv\").squeeze()\n",
    "y_val = pd.read_csv(DATA_PATH / \"y_val.csv\").squeeze()\n",
    "y_test = pd.read_csv(DATA_PATH / \"y_test.csv\").squeeze()\n",
    "\n",
    "print(f\"X_train: {X_train.shape}\")\n",
    "print(f\"X_val:   {X_val.shape}\")\n",
    "print(f\"X_test:  {X_test.shape}\")\n",
    "print(f\"\\nChurn rate - Train: {y_train.mean():.2%}, Val: {y_val.mean():.2%}, Test: {y_test.mean():.2%}\")\n",
    "print(f\"\\nColumns: {list(X_train.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Feature Engineering\n",
    "\n",
    "Create 14 new features from raw data. All statistics (medians, etc.) are computed on training set only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(df, train_medians=None, fit=False):\n",
    "    \"\"\"\n",
    "    Create engineered features from raw data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame - Raw features\n",
    "    train_medians : dict - Median MonthlyCharges per InternetService (from training set)\n",
    "    fit : bool - If True, compute and return train_medians\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df_eng : pd.DataFrame - DataFrame with new features added\n",
    "    train_medians : dict - (only if fit=True)\n",
    "    \"\"\"\n",
    "    df_eng = df.copy()\n",
    "    \n",
    "    # --- Ensure TotalCharges is numeric ---\n",
    "    df_eng['TotalCharges'] = pd.to_numeric(df_eng['TotalCharges'], errors='coerce').fillna(0)\n",
    "    \n",
    "    # === 2.1 Ratio / Derived Numerical Features (5) ===\n",
    "    safe_tenure = df_eng['tenure'].clip(lower=1)\n",
    "    \n",
    "    df_eng['avg_monthly_spend'] = df_eng['TotalCharges'] / safe_tenure\n",
    "    df_eng['charge_tenure_ratio'] = df_eng['MonthlyCharges'] / safe_tenure\n",
    "    df_eng['charge_deviation'] = df_eng['MonthlyCharges'] - df_eng['avg_monthly_spend']\n",
    "    df_eng['expected_lifetime_value'] = df_eng['MonthlyCharges'] * df_eng['tenure']\n",
    "    \n",
    "    # Overpay indicator (median from training set only)\n",
    "    if fit:\n",
    "        train_medians = df_eng.groupby('InternetService')['MonthlyCharges'].median().to_dict()\n",
    "    \n",
    "    df_eng['overpay_indicator'] = (\n",
    "        df_eng['MonthlyCharges'] \n",
    "        - df_eng['InternetService'].map(train_medians)\n",
    "    )\n",
    "    \n",
    "    # === 2.2 Tenure Binning (1) ===\n",
    "    bins = [0, 6, 12, 24, 48, 72]\n",
    "    labels = [0, 1, 2, 3, 4]  # new, early, developing, established, loyal\n",
    "    df_eng['tenure_group'] = pd.cut(\n",
    "        df_eng['tenure'], bins=bins, labels=labels, include_lowest=True\n",
    "    ).astype(float)\n",
    "    \n",
    "    # === 2.3 Service Count Aggregation (2) ===\n",
    "    support_cols = ['OnlineSecurity', 'TechSupport', 'OnlineBackup', 'DeviceProtection']\n",
    "    streaming_cols = ['StreamingTV', 'StreamingMovies']\n",
    "    \n",
    "    df_eng['num_support_services'] = sum(\n",
    "        (df_eng[col] == 'Yes').astype(int) for col in support_cols\n",
    "    )\n",
    "    df_eng['num_streaming_services'] = sum(\n",
    "        (df_eng[col] == 'Yes').astype(int) for col in streaming_cols\n",
    "    )\n",
    "    \n",
    "    # === 2.4 Interaction Features (4) ===\n",
    "    df_eng['is_mtm_fiber'] = (\n",
    "        (df_eng['Contract'] == 'Month-to-month') & \n",
    "        (df_eng['InternetService'] == 'Fiber optic')\n",
    "    ).astype(int)\n",
    "    \n",
    "    df_eng['is_mtm_no_support'] = (\n",
    "        (df_eng['Contract'] == 'Month-to-month') & \n",
    "        (df_eng['num_support_services'] == 0)\n",
    "    ).astype(int)\n",
    "    \n",
    "    df_eng['is_echeck_mtm'] = (\n",
    "        (df_eng['PaymentMethod'] == 'Electronic check') & \n",
    "        (df_eng['Contract'] == 'Month-to-month')\n",
    "    ).astype(int)\n",
    "    \n",
    "    contract_ord = df_eng['Contract'].map({\n",
    "        'Month-to-month': 0, 'One year': 1, 'Two year': 2\n",
    "    })\n",
    "    df_eng['tenure_x_contract'] = df_eng['tenure'] * contract_ord\n",
    "    \n",
    "    # === 2.5 Binary Simplification (2) ===\n",
    "    df_eng['is_auto_pay'] = df_eng['PaymentMethod'].isin([\n",
    "        'Bank transfer (automatic)', 'Credit card (automatic)'\n",
    "    ]).astype(int)\n",
    "    \n",
    "    df_eng['has_internet'] = (df_eng['InternetService'] != 'No').astype(int)\n",
    "    \n",
    "    if fit:\n",
    "        return df_eng, train_medians\n",
    "    return df_eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original features: 19\n",
      "After engineering: 33\n",
      "New features (14): ['avg_monthly_spend', 'charge_tenure_ratio', 'charge_deviation', 'expected_lifetime_value', 'overpay_indicator', 'tenure_group', 'num_support_services', 'num_streaming_services', 'is_mtm_fiber', 'is_mtm_no_support', 'is_echeck_mtm', 'tenure_x_contract', 'is_auto_pay', 'has_internet']\n",
      "\n",
      "Sample of new features (first 5 rows):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg_monthly_spend</th>\n",
       "      <th>charge_tenure_ratio</th>\n",
       "      <th>charge_deviation</th>\n",
       "      <th>expected_lifetime_value</th>\n",
       "      <th>overpay_indicator</th>\n",
       "      <th>tenure_group</th>\n",
       "      <th>num_support_services</th>\n",
       "      <th>num_streaming_services</th>\n",
       "      <th>is_mtm_fiber</th>\n",
       "      <th>is_mtm_no_support</th>\n",
       "      <th>is_echeck_mtm</th>\n",
       "      <th>tenure_x_contract</th>\n",
       "      <th>is_auto_pay</th>\n",
       "      <th>has_internet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60.870455</td>\n",
       "      <td>0.926515</td>\n",
       "      <td>0.279545</td>\n",
       "      <td>4035.90</td>\n",
       "      <td>4.90</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>132</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>83.987692</td>\n",
       "      <td>1.305385</td>\n",
       "      <td>0.862308</td>\n",
       "      <td>5515.25</td>\n",
       "      <td>28.60</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>130</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.811111</td>\n",
       "      <td>0.282639</td>\n",
       "      <td>1.538889</td>\n",
       "      <td>1465.20</td>\n",
       "      <td>0.20</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>144</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>72.841912</td>\n",
       "      <td>1.072794</td>\n",
       "      <td>0.108088</td>\n",
       "      <td>4960.60</td>\n",
       "      <td>16.70</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>136</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36.020833</td>\n",
       "      <td>2.958333</td>\n",
       "      <td>-0.520833</td>\n",
       "      <td>426.00</td>\n",
       "      <td>-20.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   avg_monthly_spend  charge_tenure_ratio  charge_deviation  \\\n",
       "0          60.870455             0.926515          0.279545   \n",
       "1          83.987692             1.305385          0.862308   \n",
       "2          18.811111             0.282639          1.538889   \n",
       "3          72.841912             1.072794          0.108088   \n",
       "4          36.020833             2.958333         -0.520833   \n",
       "\n",
       "   expected_lifetime_value  overpay_indicator  tenure_group  \\\n",
       "0                  4035.90               4.90           4.0   \n",
       "1                  5515.25              28.60           4.0   \n",
       "2                  1465.20               0.20           4.0   \n",
       "3                  4960.60              16.70           4.0   \n",
       "4                   426.00             -20.75           1.0   \n",
       "\n",
       "   num_support_services  num_streaming_services  is_mtm_fiber  \\\n",
       "0                     3                       0             0   \n",
       "1                     4                       2             0   \n",
       "2                     0                       0             0   \n",
       "3                     2                       2             0   \n",
       "4                     2                       0             0   \n",
       "\n",
       "   is_mtm_no_support  is_echeck_mtm  tenure_x_contract  is_auto_pay  \\\n",
       "0                  0              0                132            1   \n",
       "1                  0              0                130            1   \n",
       "2                  0              0                144            0   \n",
       "3                  0              0                136            1   \n",
       "4                  0              0                  0            0   \n",
       "\n",
       "   has_internet  \n",
       "0             1  \n",
       "1             1  \n",
       "2             0  \n",
       "3             1  \n",
       "4             1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply feature engineering (fit on train, transform val/test)\n",
    "X_train_eng, train_medians = engineer_features(X_train, fit=True)\n",
    "X_val_eng = engineer_features(X_val, train_medians=train_medians)\n",
    "X_test_eng = engineer_features(X_test, train_medians=train_medians)\n",
    "\n",
    "new_features = [\n",
    "    'avg_monthly_spend', 'charge_tenure_ratio', 'charge_deviation',\n",
    "    'expected_lifetime_value', 'overpay_indicator', 'tenure_group',\n",
    "    'num_support_services', 'num_streaming_services',\n",
    "    'is_mtm_fiber', 'is_mtm_no_support', 'is_echeck_mtm',\n",
    "    'tenure_x_contract', 'is_auto_pay', 'has_internet'\n",
    "]\n",
    "\n",
    "print(f\"Original features: {X_train.shape[1]}\")\n",
    "print(f\"After engineering: {X_train_eng.shape[1]}\")\n",
    "print(f\"New features ({len(new_features)}): {new_features}\")\n",
    "print(f\"\\nSample of new features (first 5 rows):\")\n",
    "X_train_eng[new_features].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation of new features with Churn:\n",
      "is_mtm_fiber               0.424005\n",
      "is_echeck_mtm              0.377394\n",
      "charge_tenure_ratio        0.374263\n",
      "tenure_group              -0.356248\n",
      "tenure_x_contract         -0.343074\n",
      "is_mtm_no_support          0.254760\n",
      "overpay_indicator         -0.242903\n",
      "has_internet               0.231284\n",
      "is_auto_pay               -0.218645\n",
      "avg_monthly_spend          0.197701\n",
      "expected_lifetime_value   -0.191795\n",
      "num_support_services      -0.169072\n",
      "num_streaming_services     0.074541\n",
      "charge_deviation          -0.012696\n"
     ]
    }
   ],
   "source": [
    "# Quick sanity check: correlation of new features with churn\n",
    "train_with_target = X_train_eng[new_features].copy()\n",
    "train_with_target['Churn'] = y_train.values\n",
    "\n",
    "correlations = train_with_target.corr()['Churn'].drop('Churn').sort_values(key=abs, ascending=False)\n",
    "print(\"Correlation of new features with Churn:\")\n",
    "print(correlations.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Feature Set Configurations\n",
    "\n",
    "| Config | Description | For |\n",
    "|--------|-------------|-----|\n",
    "| A | 7 current features + 14 engineered | LR |\n",
    "| B | All 19 raw + 14 engineered | Tree-based models |\n",
    "| C | 11 raw + 14 engineered | Compromise |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config A: 21 features\n",
      "Config B: 33 features\n",
      "Config C: 25 features\n"
     ]
    }
   ],
   "source": [
    "# Define feature groups\n",
    "FEATURES_CURRENT_7 = [\n",
    "    'tenure', 'MonthlyCharges', 'Contract', 'InternetService',\n",
    "    'OnlineSecurity', 'TechSupport', 'PaymentMethod'\n",
    "]\n",
    "\n",
    "FEATURES_ALL_RAW = [\n",
    "    'gender', 'SeniorCitizen', 'Partner', 'Dependents', 'tenure',\n",
    "    'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity',\n",
    "    'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV',\n",
    "    'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod',\n",
    "    'MonthlyCharges', 'TotalCharges'\n",
    "]\n",
    "\n",
    "FEATURES_MODERATE = [\n",
    "    'tenure', 'MonthlyCharges', 'TotalCharges', 'SeniorCitizen',\n",
    "    'Dependents', 'PaperlessBilling', 'Contract', 'InternetService',\n",
    "    'OnlineSecurity', 'TechSupport', 'PaymentMethod'\n",
    "]\n",
    "\n",
    "ENGINEERED = new_features\n",
    "\n",
    "# Build config sets\n",
    "config_A = FEATURES_CURRENT_7 + ENGINEERED\n",
    "config_B = FEATURES_ALL_RAW + ENGINEERED\n",
    "config_C = FEATURES_MODERATE + ENGINEERED\n",
    "\n",
    "# Remove duplicates while preserving order\n",
    "config_A = list(dict.fromkeys(config_A))\n",
    "config_B = list(dict.fromkeys(config_B))\n",
    "config_C = list(dict.fromkeys(config_C))\n",
    "\n",
    "print(f\"Config A: {len(config_A)} features\")\n",
    "print(f\"Config B: {len(config_B)} features\")\n",
    "print(f\"Config C: {len(config_C)} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Preprocessing Pipelines\n",
    "\n",
    "Two pipelines:\n",
    "- **Pipeline-linear**: StandardScaler + OneHotEncoder (for LR)\n",
    "- **Pipeline-tree**: No scaling + OrdinalEncoder (for tree-based models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline builders defined.\n"
     ]
    }
   ],
   "source": [
    "def identify_column_types(df, feature_list):\n",
    "    \"\"\"Identify numerical and categorical columns in the feature list.\"\"\"\n",
    "    num_cols = []\n",
    "    cat_cols = []\n",
    "    for col in feature_list:\n",
    "        if df[col].dtype in ['float64', 'int64', 'float32', 'int32']:\n",
    "            num_cols.append(col)\n",
    "        else:\n",
    "            cat_cols.append(col)\n",
    "    return num_cols, cat_cols\n",
    "\n",
    "\n",
    "def build_linear_pipeline(df, feature_list):\n",
    "    \"\"\"Build preprocessing pipeline for linear models (StandardScaler + OneHotEncoder).\"\"\"\n",
    "    num_cols, cat_cols = identify_column_types(df, feature_list)\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), num_cols),\n",
    "            ('cat', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='infrequent_if_exist'), cat_cols)\n",
    "        ],\n",
    "        remainder='drop'\n",
    "    )\n",
    "    return preprocessor, num_cols, cat_cols\n",
    "\n",
    "\n",
    "def build_tree_pipeline(df, feature_list):\n",
    "    \"\"\"Build preprocessing pipeline for tree-based models (OrdinalEncoder, no scaling).\"\"\"\n",
    "    num_cols, cat_cols = identify_column_types(df, feature_list)\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', 'passthrough', num_cols),\n",
    "            ('cat', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1), cat_cols)\n",
    "        ],\n",
    "        remainder='drop'\n",
    "    )\n",
    "    return preprocessor, num_cols, cat_cols\n",
    "\n",
    "\n",
    "print(\"Pipeline builders defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config B (tree): 33 features after encoding\n",
      "  Numerical: 18, Categorical: 15\n",
      "\n",
      "Config A (linear): 27 features after encoding\n",
      "Config C (linear): 31 features after encoding\n"
     ]
    }
   ],
   "source": [
    "# Build and fit preprocessing for Config B (tree-based, full features)\n",
    "tree_preprocessor_B, num_B, cat_B = build_tree_pipeline(X_train_eng, config_B)\n",
    "tree_preprocessor_B.fit(X_train_eng[config_B])\n",
    "\n",
    "X_train_tree_B = tree_preprocessor_B.transform(X_train_eng[config_B])\n",
    "X_val_tree_B = tree_preprocessor_B.transform(X_val_eng[config_B])\n",
    "X_test_tree_B = tree_preprocessor_B.transform(X_test_eng[config_B])\n",
    "\n",
    "print(f\"Config B (tree): {X_train_tree_B.shape[1]} features after encoding\")\n",
    "print(f\"  Numerical: {len(num_B)}, Categorical: {len(cat_B)}\")\n",
    "\n",
    "# Build and fit preprocessing for Config A (linear, minimal)\n",
    "linear_preprocessor_A, num_A, cat_A = build_linear_pipeline(X_train_eng, config_A)\n",
    "linear_preprocessor_A.fit(X_train_eng[config_A])\n",
    "\n",
    "X_train_lin_A = linear_preprocessor_A.transform(X_train_eng[config_A])\n",
    "X_val_lin_A = linear_preprocessor_A.transform(X_val_eng[config_A])\n",
    "X_test_lin_A = linear_preprocessor_A.transform(X_test_eng[config_A])\n",
    "\n",
    "print(f\"\\nConfig A (linear): {X_train_lin_A.shape[1]} features after encoding\")\n",
    "\n",
    "# Build and fit preprocessing for Config C (linear, moderate)\n",
    "linear_preprocessor_C, num_C, cat_C = build_linear_pipeline(X_train_eng, config_C)\n",
    "linear_preprocessor_C.fit(X_train_eng[config_C])\n",
    "\n",
    "X_train_lin_C = linear_preprocessor_C.transform(X_train_eng[config_C])\n",
    "X_val_lin_C = linear_preprocessor_C.transform(X_val_eng[config_C])\n",
    "X_test_lin_C = linear_preprocessor_C.transform(X_test_eng[config_C])\n",
    "\n",
    "print(f\"Config C (linear): {X_train_lin_C.shape[1]} features after encoding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Cross-Validation Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation framework ready.\n"
     ]
    }
   ],
   "source": [
    "# Same CV strategy as all previous notebooks\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "scoring = {\n",
    "    'roc_auc': 'roc_auc',\n",
    "    'recall': 'recall',\n",
    "    'precision': 'precision',\n",
    "    'f1': 'f1'\n",
    "}\n",
    "\n",
    "# Results storage\n",
    "all_results = []\n",
    "\n",
    "def evaluate_model(name, model, X_tr, y_tr, X_v, y_v):\n",
    "    \"\"\"Evaluate a model with CV + validation set. Returns results dict.\"\"\"\n",
    "    # Cross-validation\n",
    "    cv_scores = cross_validate(\n",
    "        model, X_tr, y_tr, cv=cv, scoring=scoring,\n",
    "        return_train_score=True, n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Fit on full training set, evaluate on validation\n",
    "    model.fit(X_tr, y_tr)\n",
    "    y_val_proba = model.predict_proba(X_v)[:, 1]\n",
    "    y_train_proba = model.predict_proba(X_tr)[:, 1]\n",
    "    \n",
    "    val_auc = roc_auc_score(y_v, y_val_proba)\n",
    "    train_auc = roc_auc_score(y_tr, y_train_proba)\n",
    "    \n",
    "    result = {\n",
    "        'Model': name,\n",
    "        'CV_AUC': cv_scores['test_roc_auc'].mean(),\n",
    "        'CV_AUC_std': cv_scores['test_roc_auc'].std(),\n",
    "        'CV_Recall': cv_scores['test_recall'].mean(),\n",
    "        'CV_Precision': cv_scores['test_precision'].mean(),\n",
    "        'CV_F1': cv_scores['test_f1'].mean(),\n",
    "        'Val_AUC': val_auc,\n",
    "        'Train_AUC': train_auc,\n",
    "        'Overfit_Gap': train_auc - val_auc\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"  CV AUC:      {result['CV_AUC']:.4f} +/- {result['CV_AUC_std']:.4f}\")\n",
    "    print(f\"  CV Recall:   {result['CV_Recall']:.4f}\")\n",
    "    print(f\"  CV Precision:{result['CV_Precision']:.4f}\")\n",
    "    print(f\"  Val AUC:     {result['Val_AUC']:.4f}\")\n",
    "    print(f\"  Train AUC:   {result['Train_AUC']:.4f}\")\n",
    "    print(f\"  Overfit Gap: {result['Overfit_Gap']:.4f} {'⚠️' if result['Overfit_Gap'] > 0.05 else '✓'}\")\n",
    "    \n",
    "    return result, model\n",
    "\n",
    "print(\"Evaluation framework ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Baseline Reference: Logistic Regression (Engineered Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "LR (Config A: 7 raw + engineered)\n",
      "============================================================\n",
      "  CV AUC:      0.8453 +/- 0.0154\n",
      "  CV Recall:   0.7743\n",
      "  CV Precision:0.5213\n",
      "  Val AUC:     0.8576\n",
      "  Train AUC:   0.8486\n",
      "  Overfit Gap: -0.0090 ✓\n"
     ]
    }
   ],
   "source": [
    "# LR with Config A (minimal + engineered)\n",
    "lr_A = LogisticRegression(C=1, penalty='l1', solver='saga', class_weight='balanced',\n",
    "                          max_iter=1000, random_state=RANDOM_STATE)\n",
    "result_lr_A, fitted_lr_A = evaluate_model(\n",
    "    \"LR (Config A: 7 raw + engineered)\", lr_A,\n",
    "    X_train_lin_A, y_train, X_val_lin_A, y_val\n",
    ")\n",
    "all_results.append(result_lr_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "LR (Config C: 11 raw + engineered)\n",
      "============================================================\n",
      "  CV AUC:      0.8471 +/- 0.0170\n",
      "  CV Recall:   0.7895\n",
      "  CV Precision:0.5301\n",
      "  Val AUC:     0.8613\n",
      "  Train AUC:   0.8514\n",
      "  Overfit Gap: -0.0099 ✓\n"
     ]
    }
   ],
   "source": [
    "# LR with Config C (moderate + engineered)\n",
    "lr_C = LogisticRegression(C=1, penalty='l1', solver='saga', class_weight='balanced',\n",
    "                          max_iter=1000, random_state=RANDOM_STATE)\n",
    "result_lr_C, fitted_lr_C = evaluate_model(\n",
    "    \"LR (Config C: 11 raw + engineered)\", lr_C,\n",
    "    X_train_lin_C, y_train, X_val_lin_C, y_val\n",
    ")\n",
    "all_results.append(result_lr_C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scale_pos_weight: 2.77\n",
      "\n",
      "Starting XGBoost RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "\n",
      "Best CV AUC: 0.8478\n",
      "Best params: {'subsample': 1.0, 'scale_pos_weight': 1, 'reg_lambda': 5.0, 'reg_alpha': 0.01, 'n_estimators': 500, 'min_child_weight': 3, 'max_depth': 3, 'learning_rate': 0.01, 'gamma': 0.2, 'colsample_bytree': 0.6}\n"
     ]
    }
   ],
   "source": [
    "# Class imbalance ratio\n",
    "scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "print(f\"scale_pos_weight: {scale_pos_weight:.2f}\")\n",
    "\n",
    "xgb_param_dist = {\n",
    "    'n_estimators': [100, 200, 300, 500],\n",
    "    'max_depth': [3, 4, 5, 6, 7],\n",
    "    'learning_rate': [0.01, 0.03, 0.05, 0.1, 0.15],\n",
    "    'subsample': [0.7, 0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'min_child_weight': [1, 3, 5, 7],\n",
    "    'gamma': [0, 0.1, 0.2, 0.3],\n",
    "    'reg_alpha': [0, 0.01, 0.1, 1.0],\n",
    "    'reg_lambda': [0.5, 1.0, 2.0, 5.0],\n",
    "    'scale_pos_weight': [1, scale_pos_weight]\n",
    "}\n",
    "\n",
    "xgb_base = XGBClassifier(\n",
    "    random_state=RANDOM_STATE,\n",
    "    eval_metric='auc',\n",
    "    use_label_encoder=False,\n",
    "    verbosity=0\n",
    ")\n",
    "\n",
    "xgb_search = RandomizedSearchCV(\n",
    "    xgb_base, xgb_param_dist, n_iter=50,\n",
    "    scoring='roc_auc', cv=cv, random_state=RANDOM_STATE,\n",
    "    n_jobs=-1, verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nStarting XGBoost RandomizedSearchCV...\")\n",
    "xgb_search.fit(X_train_tree_B, y_train)\n",
    "\n",
    "print(f\"\\nBest CV AUC: {xgb_search.best_score_:.4f}\")\n",
    "print(f\"Best params: {xgb_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "XGBoost (Config B)\n",
      "============================================================\n",
      "  CV AUC:      0.8478 +/- 0.0159\n",
      "  CV Recall:   0.5326\n",
      "  CV Precision:0.6815\n",
      "  Val AUC:     0.8607\n",
      "  Train AUC:   0.8700\n",
      "  Overfit Gap: 0.0093 ✓\n"
     ]
    }
   ],
   "source": [
    "# Evaluate best XGBoost\n",
    "best_xgb = xgb_search.best_estimator_\n",
    "result_xgb, fitted_xgb = evaluate_model(\n",
    "    \"XGBoost (Config B)\", best_xgb,\n",
    "    X_train_tree_B, y_train, X_val_tree_B, y_val\n",
    ")\n",
    "all_results.append(result_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting LightGBM RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "\n",
      "Best CV AUC: 0.8473\n",
      "Best params: {'subsample': 1.0, 'reg_lambda': 1.0, 'reg_alpha': 0.1, 'num_leaves': 31, 'n_estimators': 500, 'min_split_gain': 0.01, 'min_child_samples': 10, 'max_depth': 3, 'learning_rate': 0.01, 'is_unbalance': False, 'colsample_bytree': 0.7}\n"
     ]
    }
   ],
   "source": [
    "lgbm_param_dist = {\n",
    "    'n_estimators': [100, 200, 300, 500],\n",
    "    'max_depth': [3, 5, 7, -1],\n",
    "    'num_leaves': [15, 31, 50, 70],\n",
    "    'learning_rate': [0.01, 0.03, 0.05, 0.1],\n",
    "    'subsample': [0.7, 0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.7, 0.8, 0.9],\n",
    "    'min_child_samples': [10, 20, 30, 50],\n",
    "    'reg_alpha': [0, 0.01, 0.1, 1.0],\n",
    "    'reg_lambda': [0, 0.1, 1.0, 5.0],\n",
    "    'is_unbalance': [True, False],\n",
    "    'min_split_gain': [0, 0.01, 0.1]\n",
    "}\n",
    "\n",
    "lgbm_base = LGBMClassifier(\n",
    "    random_state=RANDOM_STATE,\n",
    "    verbosity=-1\n",
    ")\n",
    "\n",
    "lgbm_search = RandomizedSearchCV(\n",
    "    lgbm_base, lgbm_param_dist, n_iter=50,\n",
    "    scoring='roc_auc', cv=cv, random_state=RANDOM_STATE,\n",
    "    n_jobs=-1, verbose=1\n",
    ")\n",
    "\n",
    "print(\"Starting LightGBM RandomizedSearchCV...\")\n",
    "lgbm_search.fit(X_train_tree_B, y_train)\n",
    "\n",
    "print(f\"\\nBest CV AUC: {lgbm_search.best_score_:.4f}\")\n",
    "print(f\"Best params: {lgbm_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "LightGBM (Config B)\n",
      "============================================================\n",
      "  CV AUC:      0.8473 +/- 0.0154\n",
      "  CV Recall:   0.5299\n",
      "  CV Precision:0.6776\n",
      "  Val AUC:     0.8598\n",
      "  Train AUC:   0.8723\n",
      "  Overfit Gap: 0.0124 ✓\n"
     ]
    }
   ],
   "source": [
    "# Evaluate best LightGBM\n",
    "best_lgbm = lgbm_search.best_estimator_\n",
    "result_lgbm, fitted_lgbm = evaluate_model(\n",
    "    \"LightGBM (Config B)\", best_lgbm,\n",
    "    X_train_tree_B, y_train, X_val_tree_B, y_val\n",
    ")\n",
    "all_results.append(result_lgbm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Random Forest (Properly Tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Random Forest RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "\n",
      "Best CV AUC: 0.8482\n",
      "Best params: {'n_estimators': 300, 'min_samples_split': 15, 'min_samples_leaf': 10, 'max_samples': 0.8, 'max_features': 'sqrt', 'max_depth': 8, 'class_weight': 'balanced_subsample'}\n"
     ]
    }
   ],
   "source": [
    "rf_param_dist = {\n",
    "    'n_estimators': [200, 300, 500, 700],\n",
    "    'max_depth': [6, 8, 10, 12, None],\n",
    "    'min_samples_split': [5, 10, 15, 20],\n",
    "    'min_samples_leaf': [3, 5, 8, 10],\n",
    "    'max_features': ['sqrt', 'log2', 0.3, 0.5],\n",
    "    'class_weight': ['balanced', 'balanced_subsample'],\n",
    "    'max_samples': [0.7, 0.8, 0.9, None]\n",
    "}\n",
    "\n",
    "rf_base = RandomForestClassifier(\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_search = RandomizedSearchCV(\n",
    "    rf_base, rf_param_dist, n_iter=40,\n",
    "    scoring='roc_auc', cv=cv, random_state=RANDOM_STATE,\n",
    "    n_jobs=-1, verbose=1\n",
    ")\n",
    "\n",
    "print(\"Starting Random Forest RandomizedSearchCV...\")\n",
    "rf_search.fit(X_train_tree_B, y_train)\n",
    "\n",
    "print(f\"\\nBest CV AUC: {rf_search.best_score_:.4f}\")\n",
    "print(f\"Best params: {rf_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate best Random Forest\n",
    "best_rf = rf_search.best_estimator_\n",
    "result_rf, fitted_rf = evaluate_model(\n",
    "    \"Random Forest Tuned (Config B)\", best_rf,\n",
    "    X_train_tree_B, y_train, X_val_tree_B, y_val\n",
    ")\n",
    "all_results.append(result_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Gradient Boosting (Properly Tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_param_dist = {\n",
    "    'n_estimators': [100, 200, 300, 500],\n",
    "    'max_depth': [3, 4, 5, 6],\n",
    "    'learning_rate': [0.01, 0.03, 0.05, 0.1],\n",
    "    'subsample': [0.7, 0.8, 0.9, 1.0],\n",
    "    'min_samples_split': [10, 15, 20],\n",
    "    'min_samples_leaf': [5, 8, 10],\n",
    "    'max_features': ['sqrt', 0.3, 0.5]\n",
    "}\n",
    "\n",
    "gb_base = GradientBoostingClassifier(random_state=RANDOM_STATE)\n",
    "\n",
    "# Compute sample weights for class balance\n",
    "sample_weights = np.where(y_train == 1, scale_pos_weight, 1.0)\n",
    "\n",
    "gb_search = RandomizedSearchCV(\n",
    "    gb_base, gb_param_dist, n_iter=40,\n",
    "    scoring='roc_auc', cv=cv, random_state=RANDOM_STATE,\n",
    "    n_jobs=-1, verbose=1\n",
    ")\n",
    "\n",
    "print(\"Starting Gradient Boosting RandomizedSearchCV...\")\n",
    "gb_search.fit(X_train_tree_B, y_train, sample_weight=sample_weights)\n",
    "\n",
    "print(f\"\\nBest CV AUC: {gb_search.best_score_:.4f}\")\n",
    "print(f\"Best params: {gb_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate best Gradient Boosting\n",
    "best_gb = gb_search.best_estimator_\n",
    "result_gb, fitted_gb = evaluate_model(\n",
    "    \"Gradient Boosting Tuned (Config B)\", best_gb,\n",
    "    X_train_tree_B, y_train, X_val_tree_B, y_val\n",
    ")\n",
    "all_results.append(result_gb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile all results\n",
    "results_df = pd.DataFrame(all_results).sort_values('CV_AUC', ascending=False)\n",
    "\n",
    "print(\"=\" * 90)\n",
    "print(\"MODEL COMPARISON (sorted by CV AUC)\")\n",
    "print(\"=\" * 90)\n",
    "print(results_df.to_string(index=False, float_format='{:.4f}'.format))\n",
    "print(\"=\" * 90)\n",
    "\n",
    "# Highlight baseline comparison\n",
    "baseline_auc = 0.828\n",
    "print(f\"\\nBaseline AUC (LR, 13 features, test): {baseline_auc}\")\n",
    "print(f\"\\nModels beating baseline CV AUC:\")\n",
    "for _, row in results_df.iterrows():\n",
    "    if row['CV_AUC'] > baseline_auc:\n",
    "        print(f\"  ✓ {row['Model']}: {row['CV_AUC']:.4f} (+{(row['CV_AUC']-baseline_auc)*100:.1f} pts)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# CV AUC comparison\n",
    "models = results_df['Model'].values\n",
    "cv_aucs = results_df['CV_AUC'].values\n",
    "cv_stds = results_df['CV_AUC_std'].values\n",
    "\n",
    "colors = ['#2ecc71' if a > baseline_auc else '#e74c3c' for a in cv_aucs]\n",
    "\n",
    "axes[0].barh(models, cv_aucs, xerr=cv_stds, color=colors, alpha=0.7, edgecolor='black')\n",
    "axes[0].axvline(x=baseline_auc, color='red', linestyle='--', label=f'Baseline ({baseline_auc})')\n",
    "axes[0].axvline(x=0.85, color='blue', linestyle='--', label='Target (0.85)')\n",
    "axes[0].set_xlabel('CV ROC-AUC')\n",
    "axes[0].set_title('CV AUC Comparison', fontweight='bold')\n",
    "axes[0].legend()\n",
    "\n",
    "# Overfitting gap\n",
    "gaps = results_df['Overfit_Gap'].values\n",
    "gap_colors = ['#e74c3c' if g > 0.05 else '#2ecc71' for g in gaps]\n",
    "axes[1].barh(models, gaps, color=gap_colors, alpha=0.7, edgecolor='black')\n",
    "axes[1].axvline(x=0.05, color='red', linestyle='--', label='Overfit threshold (0.05)')\n",
    "axes[1].set_xlabel('Train - Val AUC Gap')\n",
    "axes[1].set_title('Overfitting Check', fontweight='bold')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 12. Ensemble (if applicable)\n",
    "\n",
    "Build an ensemble only if >= 2 models beat the baseline AUC of 0.828."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which models beat baseline\n",
    "winners = results_df[results_df['CV_AUC'] > baseline_auc]\n",
    "print(f\"Models beating baseline: {len(winners)}\")\n",
    "\n",
    "if len(winners) >= 2:\n",
    "    print(\"\\n→ Proceeding with ensemble strategy.\")\n",
    "    \n",
    "    # Identify the best tree-based models for ensemble\n",
    "    # (We use the fitted models from sections 7-10)\n",
    "    ensemble_candidates = {\n",
    "        'xgb': fitted_xgb,\n",
    "        'lgbm': fitted_lgbm,\n",
    "        'rf': fitted_rf\n",
    "    }\n",
    "    \n",
    "    # === 12.1 Soft Voting ===\n",
    "    voting = VotingClassifier(\n",
    "        estimators=list(ensemble_candidates.items()),\n",
    "        voting='soft'\n",
    "    )\n",
    "    result_voting, fitted_voting = evaluate_model(\n",
    "        \"Soft Voting (XGB+LGBM+RF)\", voting,\n",
    "        X_train_tree_B, y_train, X_val_tree_B, y_val\n",
    "    )\n",
    "    all_results.append(result_voting)\n",
    "    \n",
    "else:\n",
    "    print(\"\\n→ Fewer than 2 models beat baseline. Skipping ensemble.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 12.2 Stacking (if applicable) ===\n",
    "if len(winners) >= 2:\n",
    "    stacking = StackingClassifier(\n",
    "        estimators=list(ensemble_candidates.items()),\n",
    "        final_estimator=LogisticRegression(\n",
    "            max_iter=1000, class_weight='balanced', random_state=RANDOM_STATE\n",
    "        ),\n",
    "        cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE),\n",
    "        stack_method='predict_proba',\n",
    "        passthrough=False\n",
    "    )\n",
    "    \n",
    "    result_stacking, fitted_stacking = evaluate_model(\n",
    "        \"Stacking (XGB+LGBM+RF → LR)\", stacking,\n",
    "        X_train_tree_B, y_train, X_val_tree_B, y_val\n",
    "    )\n",
    "    all_results.append(result_stacking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 13. Final Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated comparison with ensembles\n",
    "final_results_df = pd.DataFrame(all_results).sort_values('CV_AUC', ascending=False)\n",
    "\n",
    "print(\"=\" * 90)\n",
    "print(\"FINAL MODEL RANKING\")\n",
    "print(\"=\" * 90)\n",
    "print(final_results_df.to_string(index=False, float_format='{:.4f}'.format))\n",
    "print(\"=\" * 90)\n",
    "\n",
    "# Select best model (highest Val AUC with Overfit_Gap < 0.05)\n",
    "safe_models = final_results_df[final_results_df['Overfit_Gap'] < 0.05]\n",
    "if len(safe_models) > 0:\n",
    "    best_row = safe_models.iloc[0]  # Already sorted by CV_AUC desc\n",
    "else:\n",
    "    best_row = final_results_df.iloc[0]\n",
    "\n",
    "print(f\"\\n✓ SELECTED: {best_row['Model']}\")\n",
    "print(f\"  CV AUC: {best_row['CV_AUC']:.4f}\")\n",
    "print(f\"  Val AUC: {best_row['Val_AUC']:.4f}\")\n",
    "print(f\"  Overfit Gap: {best_row['Overfit_Gap']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 14. Threshold Optimization (Validation Set)\n",
    "\n",
    "Same protocol as notebook 10: sweep thresholds, select one that satisfies Recall >= 80% with best Precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions from the selected best model on validation set\n",
    "# NOTE: Update this cell to use the correct fitted model variable\n",
    "# based on the selection from section 13\n",
    "\n",
    "# Map model names to fitted objects and test data\n",
    "model_registry = {\n",
    "    'XGBoost (Config B)': (fitted_xgb, X_val_tree_B, X_test_tree_B),\n",
    "    'LightGBM (Config B)': (fitted_lgbm, X_val_tree_B, X_test_tree_B),\n",
    "    'Random Forest Tuned (Config B)': (fitted_rf, X_val_tree_B, X_test_tree_B),\n",
    "    'Gradient Boosting Tuned (Config B)': (fitted_gb, X_val_tree_B, X_test_tree_B),\n",
    "    'LR (Config A: 7 raw + engineered)': (fitted_lr_A, X_val_lin_A, X_test_lin_A),\n",
    "    'LR (Config C: 11 raw + engineered)': (fitted_lr_C, X_val_lin_C, X_test_lin_C),\n",
    "}\n",
    "\n",
    "# Add ensembles if they exist\n",
    "if 'fitted_voting' in dir():\n",
    "    model_registry['Soft Voting (XGB+LGBM+RF)'] = (fitted_voting, X_val_tree_B, X_test_tree_B)\n",
    "if 'fitted_stacking' in dir():\n",
    "    model_registry['Stacking (XGB+LGBM+RF → LR)'] = (fitted_stacking, X_val_tree_B, X_test_tree_B)\n",
    "\n",
    "selected_name = best_row['Model']\n",
    "selected_model, X_val_selected, X_test_selected = model_registry[selected_name]\n",
    "\n",
    "y_val_proba = selected_model.predict_proba(X_val_selected)[:, 1]\n",
    "print(f\"Selected model: {selected_name}\")\n",
    "print(f\"Val predictions generated: {len(y_val_proba)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold sweep\n",
    "thresholds = np.arange(0.20, 0.85, 0.05)\n",
    "threshold_results = []\n",
    "\n",
    "for thresh in thresholds:\n",
    "    y_pred = (y_val_proba >= thresh).astype(int)\n",
    "    cm = confusion_matrix(y_val, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    total_churners = tp + fn\n",
    "    total_non_churners = tn + fp\n",
    "    \n",
    "    threshold_results.append({\n",
    "        'Threshold': round(thresh, 2),\n",
    "        'Precision': precision_score(y_val, y_pred, zero_division=0),\n",
    "        'Recall': recall_score(y_val, y_pred, zero_division=0),\n",
    "        'F1': f1_score(y_val, y_pred, zero_division=0),\n",
    "        'TP': tp, 'FP': fp, 'FN': fn, 'TN': tn,\n",
    "        'Churners_Captured_%': tp / total_churners * 100 if total_churners > 0 else 0,\n",
    "        'FPR_%': fp / total_non_churners * 100 if total_non_churners > 0 else 0,\n",
    "        'Total_Flagged': tp + fp\n",
    "    })\n",
    "\n",
    "df_thresh = pd.DataFrame(threshold_results).round(3)\n",
    "\n",
    "print(\"THRESHOLD ANALYSIS (Validation Set)\")\n",
    "print(\"=\" * 80)\n",
    "print(df_thresh[['Threshold', 'Precision', 'Recall', 'F1', 'Churners_Captured_%', 'FPR_%', 'Total_Flagged']].to_string(index=False))\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select optimal threshold: Recall >= 80% with highest Precision\n",
    "valid_thresholds = df_thresh[df_thresh['Recall'] >= 0.80]\n",
    "\n",
    "if len(valid_thresholds) > 0:\n",
    "    best_thresh_row = valid_thresholds.loc[valid_thresholds['Precision'].idxmax()]\n",
    "    FINAL_THRESHOLD = best_thresh_row['Threshold']\n",
    "else:\n",
    "    # Fallback: best F1\n",
    "    best_thresh_row = df_thresh.loc[df_thresh['F1'].idxmax()]\n",
    "    FINAL_THRESHOLD = best_thresh_row['Threshold']\n",
    "\n",
    "print(f\"\\n✓ SELECTED THRESHOLD: {FINAL_THRESHOLD}\")\n",
    "print(f\"  Recall:    {best_thresh_row['Recall']:.1%}\")\n",
    "print(f\"  Precision: {best_thresh_row['Precision']:.1%}\")\n",
    "print(f\"  F1:        {best_thresh_row['F1']:.3f}\")\n",
    "print(f\"  Flagged:   {int(best_thresh_row['Total_Flagged'])} customers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 15. Final Test Set Evaluation\n",
    "\n",
    "**First and only time touching the test set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set predictions\n",
    "y_test_proba = selected_model.predict_proba(X_test_selected)[:, 1]\n",
    "y_test_pred = (y_test_proba >= FINAL_THRESHOLD).astype(int)\n",
    "\n",
    "# Metrics\n",
    "test_roc_auc = roc_auc_score(y_test, y_test_proba)\n",
    "test_recall = recall_score(y_test, y_test_pred)\n",
    "test_precision = precision_score(y_test, y_test_pred)\n",
    "test_f1 = f1_score(y_test, y_test_pred)\n",
    "\n",
    "cm_test = confusion_matrix(y_test, y_test_pred)\n",
    "tn, fp, fn, tp = cm_test.ravel()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"FINAL TEST SET RESULTS — {selected_name}\")\n",
    "print(f\"Threshold: {FINAL_THRESHOLD}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n  ROC-AUC:    {test_roc_auc:.4f}  {'✓' if test_roc_auc >= 0.85 else '✗'} (target >= 0.85)\")\n",
    "print(f\"  Recall:     {test_recall:.4f}  {'✓' if test_recall >= 0.80 else '✗'} (target >= 0.80)\")\n",
    "print(f\"  Precision:  {test_precision:.4f}  {'✓' if test_precision >= 0.50 else '✗'} (target >= 0.50)\")\n",
    "print(f\"  F1 Score:   {test_f1:.4f}\")\n",
    "\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"  TP: {tp} | FP: {fp}\")\n",
    "print(f\"  FN: {fn} | TN: {tn}\")\n",
    "\n",
    "total_churners = tp + fn\n",
    "print(f\"\\nBusiness KPIs:\")\n",
    "print(f\"  Churners captured: {tp}/{total_churners} ({tp/total_churners*100:.1f}%)\")\n",
    "print(f\"  Churners missed:   {fn}\")\n",
    "print(f\"  False alarms:      {fp}\")\n",
    "print(f\"  Total flagged:     {tp+fp}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison with baseline\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"IMPROVEMENT vs BASELINE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Metric':<12} {'Baseline':>10} {'New':>10} {'Delta':>10}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'ROC-AUC':<12} {'0.8282':>10} {test_roc_auc:>10.4f} {(test_roc_auc-0.8282)*100:>+9.1f} pts\")\n",
    "print(f\"{'Recall':<12} {'0.8610':>10} {test_recall:>10.4f} {(test_recall-0.8610)*100:>+9.1f} pts\")\n",
    "print(f\"{'Precision':<12} {'0.4626':>10} {test_precision:>10.4f} {(test_precision-0.4626)*100:>+9.1f} pts\")\n",
    "print(f\"{'F1':<12} {'0.6019':>10} {test_f1:>10.4f} {(test_f1-0.6019)*100:>+9.1f} pts\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix visualization\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "total = cm_test.sum()\n",
    "labels = np.array([\n",
    "    [f'{tn}\\n({tn/total*100:.1f}%)', f'{fp}\\n({fp/total*100:.1f}%)'],\n",
    "    [f'{fn}\\n({fn/total*100:.1f}%)', f'{tp}\\n({tp/total*100:.1f}%)']\n",
    "])\n",
    "\n",
    "sns.heatmap(cm_test, annot=labels, fmt='', cmap='Blues', ax=ax,\n",
    "            xticklabels=['Predicted: Stay', 'Predicted: Churn'],\n",
    "            yticklabels=['Actual: Stay', 'Actual: Churn'])\n",
    "ax.set_title(f'Final Confusion Matrix — {selected_name} (Threshold={FINAL_THRESHOLD})',\n",
    "             fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Actual')\n",
    "ax.set_xlabel('Predicted')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 16. Save Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model and preprocessor\n",
    "joblib.dump(selected_model, MODELS_PATH / \"best_model_v2.joblib\")\n",
    "print(f\"✓ Model saved: {MODELS_PATH / 'best_model_v2.joblib'}\")\n",
    "\n",
    "# Save the appropriate preprocessor\n",
    "# Determine which preprocessor was used\n",
    "if 'Config B' in selected_name or 'Voting' in selected_name or 'Stacking' in selected_name:\n",
    "    joblib.dump(tree_preprocessor_B, MODELS_PATH / \"preprocessor_v2.joblib\")\n",
    "    print(f\"✓ Preprocessor saved: tree pipeline (Config B)\")\n",
    "elif 'Config A' in selected_name:\n",
    "    joblib.dump(linear_preprocessor_A, MODELS_PATH / \"preprocessor_v2.joblib\")\n",
    "    print(f\"✓ Preprocessor saved: linear pipeline (Config A)\")\n",
    "else:\n",
    "    joblib.dump(linear_preprocessor_C, MODELS_PATH / \"preprocessor_v2.joblib\")\n",
    "    print(f\"✓ Preprocessor saved: linear pipeline (Config C)\")\n",
    "\n",
    "# Save train medians for feature engineering\n",
    "joblib.dump(train_medians, MODELS_PATH / \"train_medians.joblib\")\n",
    "print(f\"✓ Train medians saved (for feature engineering)\")\n",
    "\n",
    "# Save final test results\n",
    "final_results = {\n",
    "    'model_type': selected_name,\n",
    "    'final_threshold': FINAL_THRESHOLD,\n",
    "    'test_set_size': len(y_test),\n",
    "    'roc_auc': test_roc_auc,\n",
    "    'precision': test_precision,\n",
    "    'recall': test_recall,\n",
    "    'f1_score': test_f1,\n",
    "    'true_positives': int(tp),\n",
    "    'false_positives': int(fp),\n",
    "    'true_negatives': int(tn),\n",
    "    'false_negatives': int(fn),\n",
    "    'evaluation_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "}\n",
    "\n",
    "pd.DataFrame([final_results]).to_csv(MODELS_PATH / 'final_test_results_v2.csv', index=False)\n",
    "print(f\"✓ Results saved: {MODELS_PATH / 'final_test_results_v2.csv'}\")\n",
    "\n",
    "# Save full comparison table\n",
    "final_results_df.to_csv(MODELS_PATH / 'model_comparison_v2.csv', index=False)\n",
    "print(f\"✓ Comparison saved: {MODELS_PATH / 'model_comparison_v2.csv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"NOTEBOOK COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nBest model: {selected_name}\")\n",
    "print(f\"Threshold:  {FINAL_THRESHOLD}\")\n",
    "print(f\"Test AUC:   {test_roc_auc:.4f}\")\n",
    "print(f\"Test Recall:{test_recall:.4f}\")\n",
    "print(f\"Test Prec:  {test_precision:.4f}\")\n",
    "print(f\"\\nArtifacts saved:\")\n",
    "print(f\"  • best_model_v2.joblib\")\n",
    "print(f\"  • preprocessor_v2.joblib\")\n",
    "print(f\"  • train_medians.joblib\")\n",
    "print(f\"  • final_test_results_v2.csv\")\n",
    "print(f\"  • model_comparison_v2.csv\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
