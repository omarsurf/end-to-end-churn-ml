{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 09 - Hyperparameter Tuning\n",
        "\n",
        "## Purpose\n",
        "\n",
        "This notebook performs hyperparameter optimization for the Logistic Regression model using a two-stage approach: RandomizedSearchCV for broad exploration followed by GridSearchCV for fine-tuning.\n",
        "\n",
        "## Configuration\n",
        "\n",
        "| Parameter | Value |\n",
        "|-----------|-------|\n",
        "| Model | Logistic Regression |\n",
        "| Scoring metric | ROC-AUC |\n",
        "| Cross-validation | Stratified 5-fold |\n",
        "| Tuning strategy | RandomizedSearch \u2192 GridSearch |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 1. Setup & Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setup complete.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import (\n",
        "    RandomizedSearchCV,\n",
        "    GridSearchCV,\n",
        "    StratifiedKFold\n",
        ")\n",
        "from sklearn.metrics import (\n",
        "    roc_auc_score,\n",
        "    precision_recall_curve,\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    f1_score,\n",
        "    precision_score,\n",
        "    recall_score\n",
        ")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Reproducibility\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "\n",
        "print(\"Setup complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 2. Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scoring metric: roc_auc\n",
            "CV folds: 5\n",
            "RandomizedSearch iterations: 30\n"
          ]
        }
      ],
      "source": [
        "# Cross-validation configuration\n",
        "CV_FOLDS = 5\n",
        "SCORING = \"roc_auc\"\n",
        "\n",
        "# RandomizedSearch configuration\n",
        "N_ITER_RANDOM = 30\n",
        "\n",
        "print(f\"Scoring metric: {SCORING}\")\n",
        "print(f\"CV folds: {CV_FOLDS}\")\n",
        "print(f\"RandomizedSearch iterations: {N_ITER_RANDOM}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 3. Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PROJECT_ROOT: /Users/omarpiro/churn_ml_decision\n",
            "DATA_PROCESSED_PATH exists: True\n",
            "MODELS_PATH exists: True\n"
          ]
        }
      ],
      "source": [
        "PROJECT_ROOT = Path(os.getcwd()).parent\n",
        "DATA_PROCESSED_PATH = PROJECT_ROOT / \"data\" / \"processed\"\n",
        "MODELS_PATH = PROJECT_ROOT / \"models\"\n",
        "\n",
        "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
        "print(\"DATA_PROCESSED_PATH exists:\", DATA_PROCESSED_PATH.exists())\n",
        "print(\"MODELS_PATH exists:\", MODELS_PATH.exists())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train shape: (4225, 31)\n",
            "X_val shape: (1409, 31)\n",
            "y_train shape: (4225,)\n",
            "y_val shape: (1409,)\n",
            "\n",
            "Number of features: 31\n"
          ]
        }
      ],
      "source": [
        "# Load processed datasets\n",
        "X_train = np.load(DATA_PROCESSED_PATH / \"X_train_processed.npy\")\n",
        "X_val = np.load(DATA_PROCESSED_PATH / \"X_val_processed.npy\")\n",
        "y_train = np.load(DATA_PROCESSED_PATH / \"y_train.npy\")\n",
        "y_val = np.load(DATA_PROCESSED_PATH / \"y_val.npy\")\n",
        "\n",
        "# Load feature names for reference\n",
        "feature_names = pd.read_csv(MODELS_PATH / \"final_feature_names.csv\")[\"feature_name\"].tolist()\n",
        "\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"X_val shape: {X_val.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"y_val shape: {y_val.shape}\")\n",
        "print(f\"\\nNumber of features: {len(feature_names)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set churn rate: 26.53%\n",
            "Validation set churn rate: 26.54%\n"
          ]
        }
      ],
      "source": [
        "# Check class distribution\n",
        "train_churn_rate = y_train.mean()\n",
        "val_churn_rate = y_val.mean()\n",
        "\n",
        "print(f\"Training set churn rate: {train_churn_rate:.2%}\")\n",
        "print(f\"Validation set churn rate: {val_churn_rate:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 4. Cross-Validation Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cross-validation strategy: StratifiedKFold\n",
            "  - n_splits: 5\n",
            "  - shuffle: True\n",
            "  - random_state: 42\n"
          ]
        }
      ],
      "source": [
        "# Stratified K-Fold to maintain class distribution\n",
        "cv_strategy = StratifiedKFold(\n",
        "    n_splits=CV_FOLDS,\n",
        "    shuffle=True,\n",
        "    random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "print(f\"Cross-validation strategy: StratifiedKFold\")\n",
        "print(f\"  - n_splits: {CV_FOLDS}\")\n",
        "print(f\"  - shuffle: True\")\n",
        "print(f\"  - random_state: {RANDOM_STATE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 5. Stage 1: RandomizedSearchCV (Exploration)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  C: 6 values\n",
            "  penalty: 2 values\n",
            "  solver: 2 values\n",
            "  class_weight: 2 values\n",
            "\n",
            "Total possible combinations: 48\n",
            "RandomizedSearch will sample: 30 combinations\n"
          ]
        }
      ],
      "source": [
        "# Define broad parameter space for exploration\n",
        "param_distributions = {\n",
        "    \"C\": [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "    \"penalty\": [\"l1\", \"l2\"],\n",
        "    \"solver\": [\"liblinear\", \"saga\"],\n",
        "    \"class_weight\": [None, \"balanced\"]\n",
        "}\n",
        "\n",
        "# Calculate total combinations\n",
        "total_combinations = 1\n",
        "for param, values in param_distributions.items():\n",
        "    total_combinations *= len(values)\n",
        "    print(f\"  {param}: {len(values)} values\")\n",
        "\n",
        "print(f\"\\nTotal possible combinations: {total_combinations}\")\n",
        "print(f\"RandomizedSearch will sample: {N_ITER_RANDOM} combinations\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RandomizedSearchCV configured.\n"
          ]
        }
      ],
      "source": [
        "# Base model\n",
        "base_model = LogisticRegression(\n",
        "    random_state=RANDOM_STATE,\n",
        "    max_iter=1000\n",
        ")\n",
        "\n",
        "# RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=base_model,\n",
        "    param_distributions=param_distributions,\n",
        "    n_iter=N_ITER_RANDOM,\n",
        "    scoring=SCORING,\n",
        "    cv=cv_strategy,\n",
        "    random_state=RANDOM_STATE,\n",
        "    n_jobs=-1,\n",
        "    verbose=1,\n",
        "    return_train_score=True\n",
        ")\n",
        "\n",
        "print(\"RandomizedSearchCV configured.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting RandomizedSearchCV...\n",
            "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
            "\n",
            "RandomizedSearchCV complete.\n"
          ]
        }
      ],
      "source": [
        "# Fit RandomizedSearchCV\n",
        "print(\"Starting RandomizedSearchCV...\")\n",
        "random_search.fit(X_train, y_train)\n",
        "print(\"\\nRandomizedSearchCV complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RandomizedSearchCV Results\n",
            "==================================================\n",
            "Best CV score (roc_auc): 0.8453\n",
            "\n",
            "Best parameters:\n",
            "  solver: saga\n",
            "  penalty: l1\n",
            "  class_weight: None\n",
            "  C: 1\n"
          ]
        }
      ],
      "source": [
        "# RandomizedSearch results\n",
        "print(\"RandomizedSearchCV Results\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Best CV score ({SCORING}): {random_search.best_score_:.4f}\")\n",
        "print(f\"\\nBest parameters:\")\n",
        "for param, value in random_search.best_params_.items():\n",
        "    print(f\"  {param}: {value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Top 5 configurations from RandomizedSearch:\n",
            "\n",
            "#1 (Rank 1)\n",
            "  CV Score: 0.8453 (+/- 0.0171)\n",
            "  Train Score: 0.8496\n",
            "  Params: {'solver': 'saga', 'penalty': 'l1', 'class_weight': None, 'C': 1}\n",
            "\n",
            "#2 (Rank 2)\n",
            "  CV Score: 0.8453 (+/- 0.0172)\n",
            "  Train Score: 0.8496\n",
            "  Params: {'solver': 'liblinear', 'penalty': 'l1', 'class_weight': None, 'C': 1}\n",
            "\n",
            "#3 (Rank 3)\n",
            "  CV Score: 0.8451 (+/- 0.0174)\n",
            "  Train Score: 0.8497\n",
            "  Params: {'solver': 'saga', 'penalty': 'l1', 'class_weight': None, 'C': 10}\n",
            "\n",
            "#4 (Rank 4)\n",
            "  CV Score: 0.8451 (+/- 0.0175)\n",
            "  Train Score: 0.8496\n",
            "  Params: {'solver': 'saga', 'penalty': 'l1', 'class_weight': 'balanced', 'C': 1}\n",
            "\n",
            "#5 (Rank 5)\n",
            "  CV Score: 0.8451 (+/- 0.0172)\n",
            "  Train Score: 0.8496\n",
            "  Params: {'solver': 'liblinear', 'penalty': 'l2', 'class_weight': None, 'C': 1}\n"
          ]
        }
      ],
      "source": [
        "# Top 5 configurations from RandomizedSearch\n",
        "random_results = pd.DataFrame(random_search.cv_results_)\n",
        "top_configs = random_results.nsmallest(5, \"rank_test_score\")[\n",
        "    [\"params\", \"mean_test_score\", \"std_test_score\", \"mean_train_score\", \"rank_test_score\"]\n",
        "].reset_index(drop=True)\n",
        "\n",
        "print(\"\\nTop 5 configurations from RandomizedSearch:\")\n",
        "for idx, row in top_configs.iterrows():\n",
        "    print(f\"\\n#{idx+1} (Rank {row['rank_test_score']})\")\n",
        "    print(f\"  CV Score: {row['mean_test_score']:.4f} (+/- {row['std_test_score']:.4f})\")\n",
        "    print(f\"  Train Score: {row['mean_train_score']:.4f}\")\n",
        "    print(f\"  Params: {row['params']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 6. Stage 2: GridSearchCV (Fine-tuning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Refined parameter grid for GridSearchCV:\n",
            "  C: [0.1, 0.5, 1, 2, 10]\n",
            "  penalty: ['l1']\n",
            "  solver: ['saga']\n",
            "  class_weight: [None]\n"
          ]
        }
      ],
      "source": [
        "# Extract best parameters from RandomizedSearch\n",
        "best_C = random_search.best_params_[\"C\"]\n",
        "best_penalty = random_search.best_params_[\"penalty\"]\n",
        "best_solver = random_search.best_params_[\"solver\"]\n",
        "best_class_weight = random_search.best_params_[\"class_weight\"]\n",
        "\n",
        "# Define refined grid around best C value\n",
        "if best_C <= 0.01:\n",
        "    C_range = [0.001, 0.005, 0.01, 0.05, 0.1]\n",
        "elif best_C >= 100:\n",
        "    C_range = [10, 50, 100, 500, 1000]\n",
        "else:\n",
        "    C_range = [best_C / 10, best_C / 2, best_C, best_C * 2, best_C * 10]\n",
        "\n",
        "# Refined parameter grid\n",
        "param_grid_refined = {\n",
        "    \"C\": C_range,\n",
        "    \"penalty\": [best_penalty],\n",
        "    \"solver\": [best_solver],\n",
        "    \"class_weight\": [best_class_weight]\n",
        "}\n",
        "\n",
        "print(\"Refined parameter grid for GridSearchCV:\")\n",
        "for param, values in param_grid_refined.items():\n",
        "    print(f\"  {param}: {values}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Starting GridSearchCV...\n",
            "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
            "\n",
            "GridSearchCV complete.\n"
          ]
        }
      ],
      "source": [
        "# GridSearchCV for fine-tuning\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=LogisticRegression(random_state=RANDOM_STATE, max_iter=1000),\n",
        "    param_grid=param_grid_refined,\n",
        "    scoring=SCORING,\n",
        "    cv=cv_strategy,\n",
        "    n_jobs=-1,\n",
        "    verbose=1,\n",
        "    return_train_score=True\n",
        ")\n",
        "\n",
        "print(\"\\nStarting GridSearchCV...\")\n",
        "grid_search.fit(X_train, y_train)\n",
        "print(\"\\nGridSearchCV complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GridSearchCV Results\n",
            "==================================================\n",
            "Best CV score (roc_auc): 0.8453\n",
            "\n",
            "Best parameters:\n",
            "  C: 1\n",
            "  class_weight: None\n",
            "  penalty: l1\n",
            "  solver: saga\n"
          ]
        }
      ],
      "source": [
        "# GridSearch results\n",
        "print(\"GridSearchCV Results\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Best CV score ({SCORING}): {grid_search.best_score_:.4f}\")\n",
        "print(f\"\\nBest parameters:\")\n",
        "for param, value in grid_search.best_params_.items():\n",
        "    print(f\"  {param}: {value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Comparison: RandomizedSearch vs GridSearch\n",
            "==================================================\n",
            "RandomizedSearch best CV score: 0.8453\n",
            "GridSearch best CV score:       0.8453\n",
            "Improvement: 0.00%\n"
          ]
        }
      ],
      "source": [
        "# Compare RandomizedSearch vs GridSearch\n",
        "print(\"\\nComparison: RandomizedSearch vs GridSearch\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"RandomizedSearch best CV score: {random_search.best_score_:.4f}\")\n",
        "print(f\"GridSearch best CV score:       {grid_search.best_score_:.4f}\")\n",
        "print(f\"Improvement: {(grid_search.best_score_ - random_search.best_score_) * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 7. Best Model Evaluation on Validation Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Model Configuration:\n",
            "  C: 1\n",
            "  penalty: l1\n",
            "  solver: saga\n",
            "  class_weight: None\n"
          ]
        }
      ],
      "source": [
        "# Get best model from GridSearch\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "print(\"Best Model Configuration:\")\n",
        "print(f\"  C: {best_model.C}\")\n",
        "print(f\"  penalty: {best_model.penalty}\")\n",
        "print(f\"  solver: {best_model.solver}\")\n",
        "print(f\"  class_weight: {best_model.class_weight}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Set Performance (threshold = 0.5)\n",
            "==================================================\n",
            "ROC-AUC: 0.8585\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    No Churn       0.84      0.91      0.88      1035\n",
            "       Churn       0.68      0.53      0.60       374\n",
            "\n",
            "    accuracy                           0.81      1409\n",
            "   macro avg       0.76      0.72      0.74      1409\n",
            "weighted avg       0.80      0.81      0.80      1409\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Predict probabilities on validation set\n",
        "y_val_proba = best_model.predict_proba(X_val)[:, 1]\n",
        "y_val_pred = best_model.predict(X_val)\n",
        "\n",
        "# Evaluation with default threshold (0.5)\n",
        "val_roc_auc = roc_auc_score(y_val, y_val_proba)\n",
        "\n",
        "print(\"Validation Set Performance (threshold = 0.5)\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"ROC-AUC: {val_roc_auc:.4f}\")\n",
        "print(f\"\\nClassification Report:\")\n",
        "print(classification_report(y_val, y_val_pred, target_names=[\"No Churn\", \"Churn\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Confusion Matrix (threshold = 0.5):\n",
            "                 Predicted\n",
            "                 No Churn  Churn\n",
            "Actual No Churn      943      92\n",
            "Actual Churn         176     198\n"
          ]
        }
      ],
      "source": [
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_val, y_val_pred)\n",
        "print(\"Confusion Matrix (threshold = 0.5):\")\n",
        "print(f\"                 Predicted\")\n",
        "print(f\"                 No Churn  Churn\")\n",
        "print(f\"Actual No Churn    {cm[0,0]:5d}   {cm[0,1]:5d}\")\n",
        "print(f\"Actual Churn       {cm[1,0]:5d}   {cm[1,1]:5d}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 8. Save best model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best model saved: /Users/omarpiro/churn_ml_decision/models/best_model.joblib\n"
          ]
        }
      ],
      "source": [
        "# Save best model\n",
        "joblib.dump(best_model, MODELS_PATH / \"best_model_tuned.joblib\")\n",
        "print(f\"Best model saved: {MODELS_PATH / 'best_model_tuned.joblib'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tuning results saved: /Users/omarpiro/churn_ml_decision/models/tuning_results.joblib\n"
          ]
        }
      ],
      "source": [
        "# Save tuning results\n",
        "tuning_results = {\n",
        "    \"best_params\": grid_search.best_params_,\n",
        "    \"best_cv_score\": grid_search.best_score_,\n",
        "    \"cv_strategy\": \"StratifiedKFold\",\n",
        "    \"cv_folds\": CV_FOLDS,\n",
        "    \"scoring\": SCORING,\n",
        "    \"random_state\": RANDOM_STATE\n",
        "}\n",
        "\n",
        "joblib.dump(tuning_results, MODELS_PATH / \"tuning_results.joblib\")\n",
        "print(f\"Tuning results saved: {MODELS_PATH / 'tuning_results.joblib'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "ARTIFACTS SUMMARY\n",
            "============================================================\n",
            "\n",
            "/Users/omarpiro/churn_ml_decision/models/\n",
            " \u251c\u2500 best_model.joblib        (Tuned Logistic Regression)\n",
            " \u2514\u2500 tuning_results.joblib    (Best hyperparameters & CV score)\n"
          ]
        }
      ],
      "source": [
        "# Summary of all saved artifacts\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ARTIFACTS SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"\\n{MODELS_PATH}/\")\n",
        "print(\" \u251c\u2500 best_model_tuned.joblib        (Tuned Logistic Regression)\")\n",
        "print(\" \u2514\u2500 tuning_results.joblib    (Best hyperparameters & CV score)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 9. Conclusion\n",
        "\n",
        "This notebook focused exclusively on hyperparameter tuning for the\n",
        "Logistic Regression model using cross-validation.\n",
        "\n",
        "- Hyperparameters were optimized using ROC-AUC as the primary metric\n",
        "- The best model configuration was selected and evaluated on the validation set\n",
        "- No decision threshold optimization was performed at this stage\n",
        "\n",
        "The tuned model and tuning metadata are saved and will be reused in the\n",
        "next notebook dedicated to business-driven threshold optimization."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}